dataset <- data$sentence
result1<- unique(dataset[grepl(paste(fileter, collapse="|"),dataset)])
write.csv(result1,"D:\\ubuntu\\WZL-project\\Project-Ford\\material.csv")
k<-4
SEED <- 2010
text.dtm <- DocumentTermMatrix(corpus)
inspect(text.dtm)
library("topicmodels")
library(tidyverse)
library(tidytext)
inspect(text.dtm)
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Docx`x`ument
text.dtm <- text.dtm[rowTotals> 0, ]
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Topic <- topics(VE, 1)
Topic
Terms<-terms(VE,20)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM4.csv")
Terms<-terms(VE,25)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM4.csv")
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Document
text.dtm <- text.dtm[rowTotals> 0, ]
Gibbs = LDA(text.dtm, k = k, method = "Gibbs",
control = list(seed = SEED, burnin = 1000,
thin = 100, iter = 1000))
Topic <- topics(Gibbs, 1)
Topic
Terms<-terms(Gibbs,20)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\Gibss4.csv")
Terms<-terms(VE,45)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM4.csv")
k<-20
SEED <- 2010
text.dtm <- DocumentTermMatrix(corpus)
inspect(text.dtm)
library(tidytext)
library("topicmodels")
library(tidyverse)
inspect(text.dtm)
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Docx`x`ument
text.dtm <- text.dtm[rowTotals> 0, ]
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Topic <- topics(VE, 1)
Topic
Terms<-terms(VE,15)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM20.csv")
install.packages("LDAvis")
library(tm)
library(xlsx)
library(rJava)
library(Rwordseg)
data<-read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\SENTENCES.csv",
header=TRUE,encoding='UTF-8',sep=",")
names(data)
dataset <- data$forR_Noun_Adj
dataset <- gsub(pattern="[@|,|;|.|?|*|!]"," ",dataset)
dataset[complete.cases(dataset)]
car <- read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\car.csv",
header=TRUE,encoding='UTF-8',sep=",")
inspect(text.dtm)
text.matrix <- as.matrix(text.dtm,encoding = "UTF-8")
len <- length(car$name)
result <- ""
for(i in seq(1, len, 50)){
result1<- dataset[grepl(paste(car[i:i+50,], collapse="|"),dataset)]
result <- c(result,result1)
}
result<-unique(result)
write.csv(result,fileEncoding='UTF-8',"D:\\ubuntu\\WZL-project\\Project-Ford\\filtering-car-term.csv")
data<-read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\filtering-car-term.csv",
header=TRUE,encoding='UTF-8',sep=",")
names(data)
dataset <- data$x
dataset <- gsub(pattern="[@|,|;|.|?|*|!]"," ",dataset)
dataset[complete.cases(dataset)]
content_source <- VectorSource(dataset)
corpus <- Corpus(content_source)
corpus <- tm_map(corpus,tolower)
corpus <- tm_map(corpus,removeWords,stopwords("german"))
corpus <- tm_map(corpus,removeWords,stopwords("en"))
text.dtm <- DocumentTermMatrix(corpus)
inspect(text.dtm)
library("topicmodels")
library(tidyverse)
library(tidytext)
k<-18
SEED <- 2010
inspect(text.dtm)
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Docx`x`ument
text.dtm <- text.dtm[rowTotals> 0, ]
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Topic <- topics(VE, 1)
Topic
Terms<-terms(VE,20)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM18.csv")
VE
inspect(VE)
name(VE)
names(VE)
VE[1]
VE[1,]
VE
theta <- t(apply(VE$document_sums + alpha, 2, function(x) x/sum(x)))
fit <- lda.collapsed.gibbs.sampler(documents = text.dtm, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
load(LDAvisData)
load(LDAvis)
install.packages("LDAvis")
load(LDAvis)
library(LDAvis)
theta <- t(apply(VE$document_sums + alpha, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
doc.list <- strsplit(dataset, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
K <- 18
set.seed(3057)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(lda)
library(LDA)
library(LDAvis)
library(LDAvis)
library("topicmodels")
install.packages("lda")
library(LDA)
library(lda)
set.seed(3057)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = carReviews$phi,
theta = carReviews$theta,
doc.length = carReviews$doc.length,
vocab = carReviews$vocab,
term.frequency = carReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
library(LDAvis)
serVis(json, out.dir = 'vis', open.browser = FALSE)
serVis(json, out.dir = 'vis', open.browser = FALSE)
serVis(json, out.dir = 'vis', open.browser = FALSE)
library(LDAvis)
doc.list <- strsplit(dataset, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)
K <- 10
G <- 5000
alpha <- 0.02
eta <- 0.02
library(lda)
set.seed(3057)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = carReviews$phi,
theta = carReviews$theta,
doc.length = carReviews$doc.length,
vocab = carReviews$vocab,
term.frequency = carReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
serVis(json, out.dir = 'vis10', open.browser = FALSE)
doc.list <- strsplit(dataset, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)
K <- 15
G <- 5000
alpha <- 0.02
eta <- 0.02
library(lda)
set.seed(3057)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = carReviews$phi,
theta = carReviews$theta,
doc.length = carReviews$doc.length,
vocab = carReviews$vocab,
term.frequency = carReviews$term.frequency)
serVis(json, out.dir = 'vis15', open.browser = FALSE)
doc.list <- strsplit(dataset, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)
K <- 4
G <- 5000
alpha <- 0.02
eta <- 0.02
library(lda)
set.seed(3057)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = carReviews$phi,
theta = carReviews$theta,
doc.length = carReviews$doc.length,
vocab = carReviews$vocab,
term.frequency = carReviews$term.frequency)
serVis(json, out.dir = 'vis4', open.browser = FALSE)
fit <- lda.collapsed.vem.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
fit <- lda.collapsed.vem.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
K <- 20
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Terms<-terms(VE,20)
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM10-2.csv")
SEED <- 2010
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Terms<-terms(VE,20)
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM10-2.csv")
text.dtm <- DocumentTermMatrix(corpus)
inspect(text.dtm)
library("topicmodels")
k<-10
SEED <- 2010
inspect(text.dtm)
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Docx`x`ument
text.dtm <- text.dtm[rowTotals > 0, ]
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Terms<-terms(VE,20)
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM10-2.csv")
set.seed(2010)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = carReviews$phi,
theta = carReviews$theta,
doc.length = carReviews$doc.length,
vocab = carReviews$vocab,
term.frequency = carReviews$term.frequency)
library(LDAvis)
doc.list <- strsplit(dataset, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
K <- 10
G <- 5000
alpha <- 0.03
eta <- 0.02
library(lda)
set.seed(2010)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(LDAvis)
doc.list <- strsplit(dataset, "[[:space:]]+")
data<-read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\filtering-car-term.csv",
header=TRUE,encoding='UTF-8',sep=",")
names(data)
dataset <- data$x
dataset <- gsub(pattern="[@|,|;|.|?|*|!]"," ",dataset)
dataset[complete.cases(dataset)]
content_source <- VectorSource(dataset)
library(lda)
set.seed(2010)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
set.seed(2010)
t1 <- Sys.time()
library(LDAvis)
doc.list <- strsplit(dataset, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)
K <- 10
G <- 5000
alpha <- 0.03
eta <- 0.02
library(lda)
set.seed(2010)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
carReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = carReviews$phi,
theta = carReviews$theta,
doc.length = carReviews$doc.length,
vocab = carReviews$vocab,
term.frequency = carReviews$term.frequency)
serVis(json, out.dir = 'vis10', open.browser = FALSE)
library("topicmodels")
library(tidyverse)
library(tidytext)
k<-10
SEED <- 2010
inspect(text.dtm)
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Docx`x`ument
text.dtm <- text.dtm[rowTotals > 0, ]
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Topic <- topics(VE, 1)
Topic
Terms<-terms(VE,20)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM10-2.csv")
content_source <- VectorSource(dataset)
corpus <- Corpus(content_source)
corpus <- tm_map(corpus,tolower)
corpus <- tm_map(corpus,removeWords,stopwords("german"))
corpus <- tm_map(corpus,removeWords,stopwords("en"))
library(tm)
library(xlsx)
library(rJava)
library(Rwordseg)
dataset <- gsub(pattern="[@|,|;|.|?|*|!]"," ",dataset)
dataset[complete.cases(dataset)]
content_source <- VectorSource(dataset)
corpus <- Corpus(content_source)
corpus <- tm_map(corpus,tolower)
corpus <- tm_map(corpus,removeWords,stopwords("german"))
corpus <- tm_map(corpus,removeWords,stopwords("en"))
text.dtm <- DocumentTermMatrix(corpus)
inspect(text.dtm)
library("topicmodels")
library(tidyverse)
library(tidytext)
k<-10
SEED <- 2010
inspect(text.dtm)
rowTotals <- apply(text.dtm , 1, sum) #Find the sum of words in each Docx`x`ument
text.dtm <- text.dtm[rowTotals > 0, ]
VE = LDA(text.dtm, k = k, control = list(seed = SEED))
Topic <- topics(VE, 1)
Topic
Terms<-terms(VE,20)
Terms
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM10-2.csv")
write.csv(Terms,"D:\\ubuntu\\WZL-project\\Project-Ford\\VEM10-2.csv")
data<-read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\SENTENCES.csv",
header=TRUE,encoding='UTF-8',sep=",")
names(data)
dataset <- data$sentence
dataset <- gsub(pattern="[@|,|;|.|?|*|!]"," ",dataset)
dataset[complete.cases(dataset)]
car <- read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\car.csv",
header=TRUE,encoding='UTF-8',sep=",")
inspect(text.dtm)
text.matrix <- as.matrix(text.dtm,encoding = "UTF-8")
for(i in seq(1, len, 50)){
result1<- dataset[grepl(paste(car[i:i+50,], collapse="|"),dataset)]
result <- c(result,result1)
}
len <- length(car$name)
result<-unique(result)
result <- ""
write.csv(result,fileEncoding='UTF-8',"D:\\ubuntu\\WZL-project\\Project-Ford\\filtering-car-term-full.csv")
dataset <- gsub(pattern="[@|,|;|.|?|*|!]"," ",dataset)
dataset[complete.cases(dataset)]
car <- read.csv("D:\\ubuntu\\WZL-project\\Project-Ford\\car.csv",
header=TRUE,encoding='UTF-8',sep=",")
inspect(text.dtm)
text.matrix <- as.matrix(text.dtm,encoding = "UTF-8")
len <- length(car$name)
result <- ""
for(i in seq(1, len, 50)){
result1<- dataset[grepl(paste(car[i:i+50,], collapse="|"),dataset)]
result <- c(result,result1)
}
result<-unique(result)
write.csv(result,fileEncoding='UTF-8',"D:\\ubuntu\\WZL-project\\Project-Ford\\filtering-car-term-full.csv")
